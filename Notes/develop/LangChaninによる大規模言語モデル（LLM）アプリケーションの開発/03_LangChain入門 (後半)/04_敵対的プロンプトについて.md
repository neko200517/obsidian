https://www.promptingguide.ai/jp/risks/adversarial

LLMを後から書き換えるような指示をして攻撃する行為のこと。プロンプトインジェクションと呼ぶ。

防御策として以下のものがある

 - 指示に防御を追加する
 - プロンプトコンポーネントのパラメータ化
 - 引用符と追加の書式
 - 敵対的プロンプトの検知
 - モデルタイプ